

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Training TensorFlow models with TPU &mdash; 简单粗暴 TensorFlow 2 0.4 alpha 文档</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../_static/js/tw_cn.js"></script>
        <script type="text/javascript" src="../../_static/js/pangu.min.js"></script>
        <script type="text/javascript" src="../../_static/js/custom.js"></script>
        <script type="text/javascript" src="../../_static/translations.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="TensorFlow Hub: Model Reuse" href="tfhub.html" />
    <link rel="prev" title="Distributed Training with TensorFlow" href="distributed.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> 简单粗暴 TensorFlow 2
          

          
          </a>

          
            
            
              <div class="version">
                0.4
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">目录</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh/preface.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh/introduction.html">TensorFlow概述</a></li>
</ul>
<p class="caption"><span class="caption-text">基础</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh/basic/installation.html">TensorFlow安装与环境配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh/basic/basic.html">TensorFlow基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh/basic/models.html">TensorFlow 模型建立与训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh/basic/tools.html">TensorFlow常用模块</a></li>
</ul>
<p class="caption"><span class="caption-text">部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh/deployment/export.html">TensorFlow模型导出</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh/deployment/serving.html">TensorFlow Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh/deployment/lite.html">TensorFlow Lite（Jinpeng）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh/deployment/javascript.html">TensorFlow in JavaScript（Huan）</a></li>
</ul>
<p class="caption"><span class="caption-text">大规模训练与加速</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh/appendix/distributed.html">TensorFlow分布式训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh/appendix/tpu.html">使用TPU训练TensorFlow模型（Huan）</a></li>
</ul>
<p class="caption"><span class="caption-text">扩展</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh/appendix/tfhub.html">TensorFlow Hub 模型复用（Jinpeng）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh/appendix/tfds.html">TensorFlow Datasets 数据集载入</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh/appendix/swift.html">Swift for TensorFlow (S4TF) (Huan）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh/appendix/julia.html">TensorFlow in Julia（Ziyang）</a></li>
</ul>
<p class="caption"><span class="caption-text">附录</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../zh/appendix/static.html">图执行模式下的 TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh/appendix/docker.html">使用Docker部署TensorFlow环境</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh/appendix/cloud.html">在云端使用TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh/appendix/jupyterlab.html">部署自己的交互式Python开发环境JupyterLab</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh/appendix/optimization.html">TensorFlow性能优化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh/appendix/recommended_books.html">参考资料与推荐阅读</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../zh/appendix/terms.html">术语中英对照表</a></li>
</ul>
<p class="caption"><span class="caption-text">Preface</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">TensorFlow Overview</a></li>
</ul>
<p class="caption"><span class="caption-text">Basic</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../basic/installation.html">Installation and Environment Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic/basic.html">TensorFlow Basic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic/models.html">Model Construction and Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic/tools.html">Common Modules in TensorFlow</a></li>
</ul>
<p class="caption"><span class="caption-text">Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deployment/export.html">TensorFlow Model Saving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/serving.html">TensorFlow Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/lite.html">TensorFlow Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/javascript.html">TensorFlow in JavaScript</a></li>
</ul>
<p class="caption"><span class="caption-text">Large-scale Training</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="distributed.html">Distributed Training with TensorFlow</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Training TensorFlow models with TPU</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction-to-tpu">Introduction to TPU</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#what-is-tpu">What is TPU?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#why-using-tpu">Why using TPU?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tpu-performance">TPU performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#tpu-environment-configuration">TPU environment configuration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#free-tpu-google-colab">Free TPU：Google Colab</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cloud-tpu">Cloud TPU</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#basic-usage-of-tpu">Basic usage of TPU</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Extensions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tfhub.html">TensorFlow Hub: Model Reuse</a></li>
<li class="toctree-l1"><a class="reference internal" href="tfds.html">TensorFlow Datasets: Ready-to-use Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="swift.html">Swift for TensorFlow (S4TF)</a></li>
<li class="toctree-l1"><a class="reference internal" href="julia.html">TensorFlow in Julia</a></li>
</ul>
<p class="caption"><span class="caption-text">Appendix</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="static.html">TensorFlow Under Graph Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="docker.html">Using Docker to deploy TensorFlow environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud.html">Using TensorFlow on cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="jupyterlab.html">Deploying Your Own Interactive Python Development Environment, JupyterLab</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">TensorFlow Performance Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="recommended_books.html">References and Recommendations for Further Reading</a></li>
<li class="toctree-l1"><a class="reference internal" href="terms.html">Terminology comparison table between Chinese and English</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">简单粗暴 TensorFlow 2</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
      <li>Training TensorFlow models with TPU</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/en/appendix/tpu.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="training-tensorflow-models-with-tpu">
<h1>Training TensorFlow models with TPU<a class="headerlink" href="#training-tensorflow-models-with-tpu" title="永久链接至标题">¶</a></h1>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/tensorflow-tpu.png"><img alt="Tensor Processing Unit - TPU" src="../../_images/tensorflow-tpu.png" style="width: 60%;" /></a>
</div>
<p>2017年5月，Alpha Go 在中国乌镇围棋峰会上，与世界第一棋士柯洁比试，并取得了三比零全胜战绩。之后的版本Alpha Zero可以通过自我学习21天即可以达到胜过中国顶尖棋手柯洁的Alpha Go Master的水平。</p>
<p>Alpha Go 背后的动力全部由 TPU 提供。TPU 使其能够更快地“思考”并在每一步之间看得更远。</p>
<div class="section" id="introduction-to-tpu">
<h2>Introduction to TPU<a class="headerlink" href="#introduction-to-tpu" title="永久链接至标题">¶</a></h2>
<div class="section" id="what-is-tpu">
<h3>What is TPU?<a class="headerlink" href="#what-is-tpu" title="永久链接至标题">¶</a></h3>
<p>TPU 代表 Tensor Processing Unit (张量处理单元) ，是由谷歌在2016年5月发布的为机器学习而构建的定制集成电路（ASIC），并为TensorFlow量身定制。</p>
<p>早在2015年，谷歌大脑团队就成立了第一个TPU中心，为 Google Translation，Photos 和 Gmail等产品提供支持。 为了使所有数据科学家和开发人员能够访问此技术，不久之后就发布了易于使用，可扩展且功能强大的基于云的TPU，以便在 Google Cloud 上运行 TensorFlow 模型。</p>
<p>TPU 由多个计算核心（Tensor Core）组成，其中包括标量，矢量和矩阵单元（MXU）。TPU（张量处理单元）与CPU（中央处理单元）和GPU（图形处理单元）最重要的区别是：TPU的硬件专为线性代数而设计，线性代数是深度学习的基石。在过去几年中，Google TPU 已经发布了 v1，v2，v3, v2 Pod, v3 Pod, Edge 等多个版本：</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>版本</p></th>
<th class="head"><p>图片</p></th>
<th class="head"><p>性能</p></th>
<th class="head"><p>内存</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>TPU (v1, 2015)</p></td>
<td><img alt="../../_images/tpu-v1.png" src="../../_images/tpu-v1.png" />
</td>
<td><p>92 TeraFLOPS</p></td>
<td><p>8 GB HBM</p></td>
</tr>
<tr class="row-odd"><td><p>Cloud TPU (v2, 2017)</p></td>
<td><img alt="../../_images/tpu-v2.jpg" src="../../_images/tpu-v2.jpg" />
</td>
<td><p>180 TeraFLOPS</p></td>
<td><p>64 GB HBM</p></td>
</tr>
<tr class="row-even"><td><p>Cloud TPU (v3, 2018)</p></td>
<td><img alt="../../_images/tpu-v3.png" src="../../_images/tpu-v3.png" />
</td>
<td><p>420 TeraFLOPS</p></td>
<td><p>128 GB HBM</p></td>
</tr>
<tr class="row-odd"><td><p>Cloud TPU Pod (v2, 2017)</p></td>
<td><img alt="../../_images/tpu-v2-pod.png" src="../../_images/tpu-v2-pod.png" />
</td>
<td><p>11,500 TeraFLOPS</p></td>
<td><p>4,096 GB HBM</p></td>
</tr>
<tr class="row-even"><td><p>Cloud TPU Pod (v3, 2018)</p></td>
<td><img alt="../../_images/tpu-v3-pod.jpg" src="../../_images/tpu-v3-pod.jpg" />
</td>
<td><p>100,000+ TeraFLOPS</p></td>
<td><p>32,768 GB HBM</p></td>
</tr>
<tr class="row-odd"><td><p>Edge TPU (Coral, 2019)</p></td>
<td><img alt="../../_images/tpu-edge-coral-usb.png" src="../../_images/tpu-edge-coral-usb.png" />
</td>
<td><p>4 TeraFLOPS</p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="simple">
<dt>注：</dt><dd><p>1. Tera: 万亿，10的12次方
1. Peta: 千万亿，10的15次方
1. FLOPS：每秒浮点数计算次数（FLoating-point Operations Per Second）
1. OPS: 每秒位整数计算次数（Integer Operations Per Second）</p>
</dd>
</dl>
<p>基于 Google Cloud，TPU 可以方便的进行建立和使用。同时，Google 也推出了专门为边缘计算环境而部署的 Edge TPU。Edge TPU 尺寸小，功耗低，性能高，可以在边缘计算环境中广泛部署高质量的AI。其作为 Cloud TPU 的补充，可以大大促进AI的解决方案在IoT环境中的部署。</p>
</div>
<div class="section" id="why-using-tpu">
<h3>Why using TPU?<a class="headerlink" href="#why-using-tpu" title="永久链接至标题">¶</a></h3>
<p>通过使用 Cloud TPU ，我们可以大大提升 TensorFlow 进行机器学习训练和预测的性能，并能够灵活的帮助研究人员，开发人员和企业 TensorFlow 计算群集。</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/tpu-pod.jpg"><img alt="TPU Pod" src="../../_images/tpu-pod.jpg" style="width: 60%;" /></a>
</div>
<p>根据 Google 提供的数据显示，在 Google Cloud TPU Pod 上可以仅用 8 分钟就能够完成ResNet-50 模型的训练。</p>
<table class="docutils align-default" id="id1">
<caption><span class="caption-text">ResNet-50</span><a class="headerlink" href="#id1" title="永久链接至表格">¶</a></caption>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>TPU</p></th>
<th class="head"><p>TPU Pod</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>训练速度（每秒图像数）</p></td>
<td><p>4000+</p></td>
<td><p>200,000+</p></td>
</tr>
<tr class="row-odd"><td><p>最终精度</p></td>
<td><p>93%</p></td>
<td><p>93%</p></td>
</tr>
<tr class="row-even"><td><p>训练时长</p></td>
<td><p>7h 47m</p></td>
<td><p>8m 45s</p></td>
</tr>
</tbody>
</table>
<p>Source: Google</p>
</div>
<div class="section" id="tpu-performance">
<h3>TPU performance<a class="headerlink" href="#tpu-performance" title="永久链接至标题">¶</a></h3>
<p>根据研究显示，TPU 比现代 GPU 和 CPU 快 15 到 30 倍。同时，TPU 还实现了比传统芯片更好的能耗效率，算力能耗比值提高了30倍至80倍。</p>
<table class="docutils align-default" id="id2">
<caption><span class="caption-text">每个周期的操作次数</span><a class="headerlink" href="#id2" title="永久链接至表格">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>CPU</p></td>
<td><p>10</p></td>
</tr>
<tr class="row-even"><td><p>GPU</p></td>
<td><p>10,000</p></td>
</tr>
<tr class="row-odd"><td><p>TPU</p></td>
<td><p>100,000</p></td>
</tr>
</tbody>
</table>
<p>Source: <a class="reference external" href="https://cloud.google.com/blog/products/gcp/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu">An in-depth look at Google’s first Tensor Processing Unit (TPU)</a></p>
</div>
</div>
<div class="section" id="tpu-environment-configuration">
<h2>TPU environment configuration<a class="headerlink" href="#tpu-environment-configuration" title="永久链接至标题">¶</a></h2>
<div class="section" id="free-tpu-google-colab">
<h3>Free TPU：Google Colab<a class="headerlink" href="#free-tpu-google-colab" title="永久链接至标题">¶</a></h3>
<p>最方便使用 TPU 的方法，就是使用 Google 的 Colab ，不但通过浏览器访问直接可以用，而且还免费。</p>
<p>在 <a class="reference external" href="https://colab.research.google.com">Google Colab</a> 的 Notebook 界面中，打开界面中，打开主菜单 <cite>Runtime</cite> ，然后选择 <cite>Change runtime type</cite>，会弹出 <cite>Notebook settings</cite> 的窗口。选择里面的 <cite>Hardware accelerator</cite> 为 <cite>TPU</cite> 就可以了。</p>
<p>为了确认 Colab Notebook 中的确分配了 TPU 资源，我们可以运行以下测试代码。如果输出 ERROR 信息，则表示目前的 Runetime 并没有分配到 TPU；如果输出 TPU 地址及设备列表，则表示 Colab 已经分配了 TPU。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pprint</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="k">if</span> <span class="s1">&#39;COLAB_TPU_ADDR&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;ERROR: Not connected to a TPU runtime&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">tpu_address</span> <span class="o">=</span> <span class="s1">&#39;grpc://&#39;</span> <span class="o">+</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;COLAB_TPU_ADDR&#39;</span><span class="p">]</span>
    <span class="k">print</span> <span class="p">(</span><span class="s1">&#39;TPU address is&#39;</span><span class="p">,</span> <span class="n">tpu_address</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">tpu_address</span><span class="p">)</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
      <span class="n">devices</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">list_devices</span><span class="p">()</span>

    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;TPU devices:&#39;</span><span class="p">)</span>
    <span class="n">pprint</span><span class="o">.</span><span class="n">pprint</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span>
</pre></div>
</div>
<p>输出信息：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TPU</span> <span class="n">address</span> <span class="ow">is</span> <span class="n">grpc</span><span class="p">:</span><span class="o">//</span><span class="mf">10.49</span><span class="o">.</span><span class="mf">237.2</span><span class="p">:</span><span class="mi">8470</span>
<span class="n">TPU</span> <span class="n">devices</span><span class="p">:</span>
<span class="p">[</span><span class="n">_DeviceAttributes</span><span class="p">(</span><span class="o">/</span><span class="n">job</span><span class="p">:</span><span class="n">tpu_worker</span><span class="o">/.../</span><span class="n">device</span><span class="p">:</span><span class="n">CPU</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="n">CPU</span><span class="p">,</span> <span class="o">...</span><span class="p">),</span>
 <span class="n">_DeviceAttributes</span><span class="p">(</span><span class="o">/</span><span class="n">job</span><span class="p">:</span><span class="n">tpu_worker</span><span class="o">/.../</span><span class="n">device</span><span class="p">:</span><span class="n">XLA_CPU</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="n">XLA_CPU</span><span class="p">,</span> <span class="o">...</span><span class="p">),</span>
 <span class="n">_DeviceAttributes</span><span class="p">(</span><span class="o">/</span><span class="n">job</span><span class="p">:</span><span class="n">tpu_worker</span><span class="o">/.../</span><span class="n">device</span><span class="p">:</span><span class="n">TPU</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="n">TPU</span><span class="p">,</span> <span class="o">...</span><span class="p">),</span>
 <span class="n">_DeviceAttributes</span><span class="p">(</span><span class="o">/</span><span class="n">job</span><span class="p">:</span><span class="n">tpu_worker</span><span class="o">/.../</span><span class="n">device</span><span class="p">:</span><span class="n">TPU</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="n">TPU</span><span class="p">,</span> <span class="o">...</span><span class="p">),</span>
 <span class="n">_DeviceAttributes</span><span class="p">(</span><span class="o">/</span><span class="n">job</span><span class="p">:</span><span class="n">tpu_worker</span><span class="o">/.../</span><span class="n">device</span><span class="p">:</span><span class="n">TPU</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="n">TPU</span><span class="p">,</span> <span class="o">...</span><span class="p">),</span>
 <span class="n">_DeviceAttributes</span><span class="p">(</span><span class="o">/</span><span class="n">job</span><span class="p">:</span><span class="n">tpu_worker</span><span class="o">/.../</span><span class="n">device</span><span class="p">:</span><span class="n">TPU</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="n">TPU</span><span class="p">,</span> <span class="o">...</span><span class="p">),</span>
 <span class="n">_DeviceAttributes</span><span class="p">(</span><span class="o">/</span><span class="n">job</span><span class="p">:</span><span class="n">tpu_worker</span><span class="o">/.../</span><span class="n">device</span><span class="p">:</span><span class="n">TPU</span><span class="p">:</span><span class="mi">4</span><span class="p">,</span> <span class="n">TPU</span><span class="p">,</span> <span class="o">...</span><span class="p">),</span>
 <span class="n">_DeviceAttributes</span><span class="p">(</span><span class="o">/</span><span class="n">job</span><span class="p">:</span><span class="n">tpu_worker</span><span class="o">/.../</span><span class="n">device</span><span class="p">:</span><span class="n">TPU</span><span class="p">:</span><span class="mi">5</span><span class="p">,</span> <span class="n">TPU</span><span class="p">,</span> <span class="o">...</span><span class="p">),</span>
 <span class="n">_DeviceAttributes</span><span class="p">(</span><span class="o">/</span><span class="n">job</span><span class="p">:</span><span class="n">tpu_worker</span><span class="o">/.../</span><span class="n">device</span><span class="p">:</span><span class="n">TPU</span><span class="p">:</span><span class="mi">6</span><span class="p">,</span> <span class="n">TPU</span><span class="p">,</span> <span class="o">...</span><span class="p">),</span>
 <span class="n">_DeviceAttributes</span><span class="p">(</span><span class="o">/</span><span class="n">job</span><span class="p">:</span><span class="n">tpu_worker</span><span class="o">/.../</span><span class="n">device</span><span class="p">:</span><span class="n">TPU</span><span class="p">:</span><span class="mi">7</span><span class="p">,</span> <span class="n">TPU</span><span class="p">,</span> <span class="o">...</span><span class="p">),</span>
 <span class="n">_DeviceAttributes</span><span class="p">(</span><span class="o">/</span><span class="n">job</span><span class="p">:</span><span class="n">tpu_worker</span><span class="o">/.../</span><span class="n">device</span><span class="p">:</span><span class="n">TPU_SYSTEM</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="n">TPU_SYSTEM</span><span class="p">,</span> <span class="o">...</span><span class="p">)]</span>
</pre></div>
</div>
<p>看到以上信息（一个CPU worker，8个TPU workers），既可以确认 Colab 的 TPU 环境设置正常。</p>
</div>
<div class="section" id="cloud-tpu">
<h3>Cloud TPU<a class="headerlink" href="#cloud-tpu" title="永久链接至标题">¶</a></h3>
<p>在 Google Cloud 上，我们可以购买所需的 TPU 资源，用来按需进行机器学习训练。为了使用 Cloud TPU ，需要在 Google Cloud Engine 中启动 VM 并为 VM 请求 Cloud TPU 资源。请求完成后，VM 就可以直接访问分配给它专属的 Cloud TPU了。</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../_images/cloud-tpu-architecture.png"><img alt="../../_images/cloud-tpu-architecture.png" src="../../_images/cloud-tpu-architecture.png" style="width: 60%;" /></a>
</div>
<p>&gt; Source: <a class="reference external" href="https://docs.google.com/presentation/d/1iodAZkOX0YMnUwohgQqNsbEkhR0zAnO-jncK9SkJ69o/edit#slide=id.g4461849552_8_3664">TPUs for Developers</a></p>
<p>在使用 Cloud TPU 时，为了免除繁琐的驱动安装，我们可以通过直接使用 Google Cloud 提供的 VM 操作系统镜像。</p>
</div>
</div>
<div class="section" id="basic-usage-of-tpu">
<h2>Basic usage of TPU<a class="headerlink" href="#basic-usage-of-tpu" title="永久链接至标题">¶</a></h2>
<p>在 TPU 上进行 TensorFlow 分布式训练的核心API是 <code class="docutils literal notranslate"><span class="pre">tf.distribute.TPUStrategy</span></code> ，可以简单几行代码就实现在 TPU 上的分布式训练，同时也可以很容易的迁移到 GPU单机多卡、多机多卡的环境。以下是如何实例化 <code class="docutils literal notranslate"><span class="pre">TPUStrategy</span></code> ：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">resolver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">resolver</span><span class="o">.</span><span class="n">TPUClusterResolver</span><span class="p">(</span>
    <span class="n">tpu</span><span class="o">=</span><span class="s1">&#39;grpc://&#39;</span> <span class="o">+</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;COLAB_TPU_ADDR&#39;</span><span class="p">])</span>
<span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental_connect_to_host</span><span class="p">(</span><span class="n">resolver</span><span class="o">.</span><span class="n">master</span><span class="p">())</span>
<span class="n">tf</span><span class="o">.</span><span class="n">tpu</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">initialize_tpu_system</span><span class="p">(</span><span class="n">resolver</span><span class="p">)</span>
<span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">TPUStrategy</span><span class="p">(</span><span class="n">resolver</span><span class="p">)</span>
</pre></div>
</div>
<p>在上面的代码中，首先我们通过 TPU 的 IP 和端口实例化 <cite>TPUClusterResolver</cite>；然后，我们通过 <cite>resolver</cite> 连接到 TPU 上，并对其进行初始化；最后，完成实例化 <cite>TPUStrategy</cite>。</p>
<p>以下使用 Fashion MNIST 分类任务展示 TPU 的使用方式。本小节的源代码可以在 <a class="reference external" href="https://github.com/huan/tensorflow-handbook-tpu">https://github.com/huan/tensorflow-handbook-tpu</a> 找到。</p>
<p>更方便的是在 Google Colab 上直接打开本例子的 Jupyter 直接运行，地址：<a class="reference external" href="https://colab.research.google.com/github/huan/tensorflow-handbook-tpu/blob/master/tensorflow-handbook-tpu-example.ipynb">https://colab.research.google.com/github/huan/tensorflow-handbook-tpu/blob/master/tensorflow-handbook-tpu-example.ipynb</a> （推荐）</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">fashion_mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>

<span class="c1"># add empty color dimension</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">create_model</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>

    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>

    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">model</span>

<span class="n">resolver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">resolver</span><span class="o">.</span><span class="n">TPUClusterResolver</span><span class="p">(</span>
    <span class="n">tpu</span><span class="o">=</span><span class="s1">&#39;grpc://&#39;</span> <span class="o">+</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;COLAB_TPU_ADDR&#39;</span><span class="p">])</span>
<span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental_connect_to_host</span><span class="p">(</span><span class="n">resolver</span><span class="o">.</span><span class="n">master</span><span class="p">())</span>
<span class="n">tf</span><span class="o">.</span><span class="n">tpu</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">initialize_tpu_system</span><span class="p">(</span><span class="n">resolver</span><span class="p">)</span>
<span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">TPUStrategy</span><span class="p">(</span><span class="n">resolver</span><span class="p">)</span>

<span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">),</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_categorical_crossentropy</span><span class="p">,</span>
        <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">sparse_categorical_accuracy</span><span class="p">])</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">x_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span>
    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">y_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
    <span class="n">validation_freq</span><span class="o">=</span><span class="mi">5</span>
<span class="p">)</span>
</pre></div>
</div>
<p>以上程序运行输出为：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">1</span><span class="o">/</span><span class="mi">5</span>
<span class="mi">60</span><span class="o">/</span><span class="mi">60</span> <span class="p">[</span><span class="o">==========</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="n">s</span> <span class="mi">23</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">12.7235</span> <span class="o">-</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.7156</span>
<span class="n">Epoch</span> <span class="mi">2</span><span class="o">/</span><span class="mi">5</span>
<span class="mi">60</span><span class="o">/</span><span class="mi">60</span> <span class="p">[</span><span class="o">==========</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="n">s</span> <span class="mi">11</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.7600</span> <span class="o">-</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.8598</span>
<span class="n">Epoch</span> <span class="mi">3</span><span class="o">/</span><span class="mi">5</span>
<span class="mi">60</span><span class="o">/</span><span class="mi">60</span> <span class="p">[</span><span class="o">==========</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="n">s</span> <span class="mi">11</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.4443</span> <span class="o">-</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.8830</span>
<span class="n">Epoch</span> <span class="mi">4</span><span class="o">/</span><span class="mi">5</span>
<span class="mi">60</span><span class="o">/</span><span class="mi">60</span> <span class="p">[</span><span class="o">==========</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="n">s</span> <span class="mi">11</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.3401</span> <span class="o">-</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.8972</span>
<span class="n">Epoch</span> <span class="mi">5</span><span class="o">/</span><span class="mi">5</span>
<span class="mi">60</span><span class="o">/</span><span class="mi">60</span> <span class="p">[</span><span class="o">==========</span><span class="p">]</span> <span class="o">-</span> <span class="mi">4</span><span class="n">s</span> <span class="mi">60</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.2867</span> <span class="o">-</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.9072</span>
<span class="mi">10</span><span class="o">/</span><span class="mi">10</span> <span class="p">[</span><span class="o">==========</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="n">s</span> <span class="mi">158</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span>
<span class="mi">10</span><span class="o">/</span><span class="mi">10</span> <span class="p">[</span><span class="o">==========</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="n">s</span> <span class="mi">158</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span>
<span class="n">val_loss</span><span class="p">:</span> <span class="mf">0.3893</span> <span class="o">-</span> <span class="n">val_sparse_categorical_accuracy</span><span class="p">:</span> <span class="mf">0.8848</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="tfhub.html" class="btn btn-neutral float-right" title="TensorFlow Hub: Model Reuse" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="distributed.html" class="btn btn-neutral float-left" title="Distributed Training with TensorFlow" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018-2019, Xihan Li（雪麒）

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>